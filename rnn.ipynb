{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN vs. FFNN\n",
    "\n",
    "There are two main differences between FFNNs and RNNs. The Recurrent Neural Network uses:\n",
    "\n",
    "1. **sequences** as inputs in the training phase, and\n",
    "2. **memory** elements\n",
    "\n",
    "Memory is defined as the output of hidden layer neurons, which will serve as additional input to the network during next training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Memory\n",
    "\n",
    "RNN’s work well for sequences of data because they have a kind of **memory**. This memory is represented by something called the **hidden state**.\n",
    "\n",
    "For example, in an character-level LSTM, each LSTM cell, in addition to accepting a character as input and generating an output character, also has some hidden state, and each cell will pass along its hidden state to the next cell.\n",
    "\n",
    "This connection creates a kind of memory by which a series of cells can remember which characters they’ve just seen and use that information to inform the next prediction!\n",
    "\n",
    "For example, if a cell has just generated the character `a` it likely will **not** generate another `a`, right after that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN vs. LSTM\n",
    "\n",
    "**RNN** have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the **vanishing gradient** or **exploding gradient** problem in which the contribution of information decays geometrically over time.\n",
    "\n",
    "**LSTM** is one option to overcome the Vanishing Gradient problem in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN (aka Elman Network)\n",
    "\n",
    "![](images/rnn-FFNN-RNN.svg)\n",
    "\n",
    "In RNNs, our output at time $t$, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time $t$ will be defined as:\n",
    "\n",
    "$$\\bar{y}_t = F(\\bar{x}_t, \\bar{x}_{t-1}, \\ldots, \\bar{x}_{t-t_0}, W)$$\n",
    "\n",
    "![multi-layer RNN](images/rnn-ML-FFNN-RNN.svg)\n",
    "\n",
    "When we train a RNN by Backpropagation Through Time, it means we first unroll the RNN in time by creating a copy of the network for each time step, viewing it as a multi-layer feedforward neural network, where the **number of layers** is equal to the **number of time steps**. Then we do backpropagation on the unrolled network, taking into account the **weight sharing**.\n",
    "\n",
    "![](images/rnn.svg)\n",
    "\n",
    "with the **input vector** $\\bar{x}$, the **output vector** $\\bar{y}$ and the **state vector** $\\bar{s}$.\n",
    "\n",
    "$W_x$ is the weight matrix connecting the inputs to the **state layer**, $W_y$ is the weight matrix connecting the state layer to the output layer and $W_s$ represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
    "\n",
    "In **FFNNs** the hidden layer depended only on the current inputs and weights, as well as on an nonlinear **activation function** $\\Phi$ like $\\text{tanh}$ or $\\text{ReLU}$:\n",
    "\n",
    "$$\\bar{h} = \\Phi(\\bar{x} W)$$\n",
    "\n",
    "In **RNNs** the state layer depended on the current inputs, their corresponding weights, the activation function and **also on the previous state**:\n",
    "\n",
    "$$\\bar{h} = \\bar{s}_t = \\Phi(\\bar{x}_t W_x + \\bar{s}_{t-1} W_s)$$\n",
    "\n",
    "The output vector is calculated exactly the same as in FFNNs. It can be a linear combination of the inputs to each output node with the corresponding weight matrix $W_y$ or a **softmax function** $\\sigma$ of the same linear combination:\n",
    "\n",
    "$$\\bar{y}_t = \\bar{s}_t W_y \\qquad \\text{or} \\qquad \\bar{y}_t = \\sigma(\\bar{s}_t W_y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with torch.nn.RNN\n",
    "\n",
    "`torch.nn.RNN` applies a multi-layer Elman RNN with $\\tanh$ or $\\text{ReLU}$ non-linearity to an input sequence.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$$s_t = \\tanh(W_x \\, x_t + b_x + W_s \\, s_{t-1} + b_s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(4, 2, batch_first=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# suppose we have a one hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# and the sequence length for the word 'hello' is 5\n",
    "seq_len = 5\n",
    "\n",
    "# single layer RNN cell with input [4] -> output [2]\n",
    "rnn = nn.RNN(input_size = 4, hidden_size = 2, num_layers = 1, batch_first = True)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of `batch_first` is to **switch** the **timestep dimension** and the **batch dimension** of the **input** and **output**. \n",
    "\n",
    "The **hidden state** has no timestep dimension, hence `batch_first` has no affect on the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 4])\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[h, e, l, l, o]], dtype = torch.float)              # [batch, seq_len, input_size] for batch_first=True\n",
    "\n",
    "print(inputs.size())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2])\n",
      "tensor([[[0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 'hidden' is the state vector\n",
    "hidden = torch.zeros(1, 1, 2)            # [num_layers, batch, hidden_size]\n",
    "\n",
    "print(hidden.size())\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rnn-Example.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = rnn(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7497, -0.6135],\n",
       "         [-0.5753, -0.0070],\n",
       "         [-0.9077, -0.3205],\n",
       "         [-0.9141, -0.2142],\n",
       "         [-0.8996,  0.3307]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'out' contains all hidden states 's1' ... 's5' throughout the sequence\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8996,  0.3307]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'hidden' is the most recent hidden state (same as 'out[:,-1,:]')\n",
    "print(hidden.shape)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s5 = tensor([[-0.8996,  0.3307]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "# weight matrix\n",
    "Ws = rnn.weight_hh_l0                            # [2,2]\n",
    "Wx = rnn.weight_ih_l0                            # [2,4]\n",
    "\n",
    "# bias vector\n",
    "Bs = rnn.bias_hh_l0.view(-1, 1)                  # [2] -> [2,1]\n",
    "Bx = rnn.bias_ih_l0.view(-1, 1)                  # [2] -> [2,1]\n",
    "\n",
    "# input\n",
    "X = inputs.squeeze(0)                            # [1,5,4] -> [5,4]\n",
    "x1 = X[0].view(-1, 1)                            #     [4] -> [4,1]\n",
    "x2 = X[1].view(-1, 1)\n",
    "x3 = X[2].view(-1, 1)\n",
    "x4 = X[3].view(-1, 1)\n",
    "x5 = X[4].view(-1, 1)\n",
    "\n",
    "# hidden states\n",
    "hidden = torch.zeros(1, 1, 2)\n",
    "s0 = hidden.squeeze(0).T                         # [1,1,2] -> [1,2] -> [2,1]\n",
    "s1 = torch.tanh(Ws @ s0 + Bs + Wx @ x1 + Bx)     # [2,1]\n",
    "s2 = torch.tanh(Ws @ s1 + Bs + Wx @ x2 + Bx)\n",
    "s3 = torch.tanh(Ws @ s2 + Bs + Wx @ x3 + Bx)\n",
    "s4 = torch.tanh(Ws @ s3 + Bs + Wx @ x4 + Bx)\n",
    "s5 = torch.tanh(Ws @ s4 + Bs + Wx @ x5 + Bx)\n",
    "\n",
    "print(f's5 = {s5.view(1,-1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the error calculations we use a **loss function**, where $E_t$ represents the **output error** at time $t$, $\\bar{d}_t$ represents the **desired output** at time $t$ and $\\bar{y}_t$ represents the calculated **output** at time $t$:\n",
    "\n",
    "$$E_t = (\\bar{d}_t - \\bar{y}_t)^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we calculate the output $\\bar{y}_t = \\bar{s}_t W_y$ and calculate the gradients at time $t = N$.\n",
    "\n",
    "**Gradient Calculations to adjust Output Weight Matrix $W_y$**:\n",
    "\n",
    "![](images/rnn-BPTT_Wy.svg)\n",
    "\n",
    "$$\\frac{\\partial{E_N}}{\\partial{W_y}} = \\frac{\\partial{E_N}}{\\partial{\\bar{y}_N}} \\cdot \\frac{\\partial{\\bar{y}_N}}{\\partial{W_y}}$$\n",
    "\n",
    "**Gradient Calculations to adjust State Weight Matrix $W_s$**:\n",
    "\n",
    "to find the final gradient calculation, we **accumulate** the contributions from **all states**: $\\bar{s}_t, \\bar{s}_{t-1}, ..., \\bar{s}_{t-N}$:\n",
    "\n",
    "![](images/rnn-BPTT_Ws.svg)\n",
    "\n",
    "$$\\frac{\\partial{E_N}}{\\partial{W_s}} = \\sum_{i=1}^{N} \\frac{\\partial{E_N}}{\\partial{\\bar{y}_N}} \\cdot \\frac{\\partial{\\bar{y}_N}}{\\partial{\\bar{s}_i}} \\cdot \\frac{\\partial{\\bar{s}_i}}{\\partial{W_s}}$$\n",
    "\n",
    "**Gradient Calculations to adjust Input Weight Matrix $W_x$**:\n",
    "\n",
    "to find the final gradient calculation, we **accumulate** the contributions from **all inputs**: $\\bar{x}_t, \\bar{x}_{t-1}, ..., \\bar{x}_{t-N}$:\n",
    "\n",
    "![](images/rnn-BPTT_Wx.svg)\n",
    "\n",
    "$$\\frac{\\partial{E_N}}{\\partial{W_x}} = \\sum_{i=1}^{N} \\frac{\\partial{E_N}}{\\partial{\\bar{y}_N}} \\cdot \\frac{\\partial{\\bar{y}_N}}{\\partial{\\bar{s}_i}} \\cdot \\frac{\\partial{\\bar{s}_i}}{\\partial{W_x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Teach a RNN to produce *ihello* from *hihell* using `torch.nn.RNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummaryX import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index to character mapping\n",
    "idx2char= ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]    # hihell\n",
    "\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "inputs = torch.tensor(x_one_hot, dtype=torch.float)\n",
    "labels = torch.tensor([1, 0, 2, 3, 3, 4], dtype=torch.long) # ihello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 3, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "We choose the following hyperparameters:\n",
    "\n",
    "* **seq_len = 6**: `|hihell| == 6`, equivalent to time steps\n",
    "* **input_size = 5**: one-hot size\n",
    "* **batch_size = 1**: one sentence per batch\n",
    "* **num_layers = 1**: one-layer rnn\n",
    "* **num_classes = 5**: predicting 5 distinct characters\n",
    "* **hidden_size = 4**: output from the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "         Kernel Shape Output Shape  Params  Mult-Adds\n",
      "Layer                                                \n",
      "0_rnn               -    [1, 6, 4]      44         36\n",
      "1_linear       [4, 5]       [6, 5]      25         20\n",
      "-----------------------------------------------------\n",
      "                      Totals\n",
      "Total params              69\n",
      "Trainable params          69\n",
      "Non-trainable params       0\n",
      "Mult-Adds                 56\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "class RNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size = 5, hidden_size = 4, num_layers = 1, batch_first = True)\n",
    "        self.linear = nn.Linear(in_features = 4, out_features = 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(1, 1, 4)            # [num_layers, batch, hidden_size]\n",
    "        x, _ = self.rnn(x, hidden)               # [1, 6, 4]\n",
    "        x = x.view(-1, 4)                        # [6, 4]\n",
    "        x = self.linear(x)                       # [6, 5]\n",
    "        return x\n",
    "\n",
    "rnn1 = RNN1()\n",
    "\n",
    "summary(rnn1, inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, loss: 1.611, predicted string: oooooo\n",
      "epoch:  2, loss: 1.436, predicted string: ooolll\n",
      "epoch:  3, loss: 1.281, predicted string: eeelll\n",
      "epoch:  4, loss: 1.133, predicted string: eeelll\n",
      "epoch:  5, loss: 0.986, predicted string: ehelll\n",
      "epoch:  6, loss: 0.838, predicted string: ehelll\n",
      "epoch:  7, loss: 0.709, predicted string: ehelll\n",
      "epoch:  8, loss: 0.594, predicted string: ehello\n",
      "epoch:  9, loss: 0.489, predicted string: ehello\n",
      "epoch: 10, loss: 0.407, predicted string: ehello\n",
      "epoch: 11, loss: 0.352, predicted string: ihello\n",
      "epoch: 12, loss: 0.310, predicted string: ihello\n",
      "epoch: 13, loss: 0.275, predicted string: ihello\n",
      "epoch: 14, loss: 0.249, predicted string: ihello\n",
      "epoch: 15, loss: 0.228, predicted string: ihello\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn1.parameters(), lr = 0.1)\n",
    "\n",
    "num_epochs = 15\n",
    "losses = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn1(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    _, idx = outputs.max(dim = 1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx]\n",
    "    print(f\"epoch: {epoch:2}, loss: {loss.item():1.3f}, predicted string: {''.join(result_str)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "As an example we have a **sequence** of single numbers (`input_size`), hence having one feature:\n",
    "```\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "```\n",
    "\n",
    "We divide it into **4 batches** (`batch_size`) of **sequence length = 5** (`seq_len`):\n",
    "\n",
    "```\n",
    "[[1, 2, 3, 4, 5],\n",
    " [6, 7, 8, 9, 10],\n",
    " [11, 12, 13, 14, 15],\n",
    " [16, 17, 18, 19, 20]]\n",
    "```\n",
    "\n",
    "Our aim is to looking at 5 (`seq_len`) previous value to **predict the next value** (`hiden_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummaryX import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21])\n",
    "data = torch.Tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "#inputs = data[:-1].view(4, 5, 1)                       # all but the last piece of data\n",
    "#labels = data[1:].view(4, 5, 1)                        # all but the first\n",
    "\n",
    "inputs = data[:-1].view(1, 5, 1)                        # all but the last piece of data\n",
    "labels = data[1:].view(1, 5, 1)                         # all but the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "      Kernel Shape Output Shape  Params  Mult-Adds\n",
      "Layer                                             \n",
      "0_rnn            -    [1, 5, 4]      28         20\n",
      "1_fc        [4, 1]       [5, 1]       5          4\n",
      "--------------------------------------------------\n",
      "                      Totals\n",
      "Total params              33\n",
      "Trainable params          33\n",
      "Non-trainable params       0\n",
      "Mult-Adds                 24\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "class RNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size = 1, hidden_size = 4, num_layers = 1, batch_first = True)\n",
    "        self.fc = nn.Linear(in_features = 4, out_features = 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = x.reshape(-1, 4)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "rnn2 = RNN2()\n",
    "\n",
    "summary(rnn2, inputs, torch.zeros(1, 1, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  2, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  3, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  4, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  5, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  6, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  7, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  8, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch:  9, loss: 21.436, predicted number: tensor([0.0316])\n",
      "epoch: 10, loss: 21.436, predicted number: tensor([0.0316])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01) \n",
    "\n",
    "num_epochs = 10\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    hidden = torch.zeros(1,1,4)\n",
    "    for batch_input in inputs:\n",
    "        batch_input = batch_input.unsqueeze(0)\n",
    "        \n",
    "        predictions, hidden = rnn2(batch_input, hidden)\n",
    "    \n",
    "        loss = criterion(predictions, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss = epoch_loss + loss.data\n",
    "    \n",
    "    # make a new variable for hidden and detach the hidden state from its history\n",
    "    # this way, we don't backpropagate through the entire history\n",
    "    #hidden = None\n",
    "    \n",
    "    epoch_loss = epoch_loss / inputs.size(0)\n",
    "    losses.append(epoch_loss)\n",
    "#    _, idx = outputs.max(dim = 1)\n",
    "#    idx = idx.data.numpy()\n",
    "#    result_str = [idx2char[c] for c in idx]\n",
    "    print(f\"epoch: {epoch:2}, loss: {epoch_loss.item():1.3f}, predicted number: {predictions.data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "TODO: https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
