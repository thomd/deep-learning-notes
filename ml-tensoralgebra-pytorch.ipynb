{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensoralgebra in PyTorch\n",
    "\n",
    "PyTorch provides the following functions for **algebraic operations**:\n",
    "\n",
    "```\n",
    "A.mm(B)                    # matrix multiplication; alias: A.matmul(B)\n",
    "A.mv(x)                    # matrix vector multiplication\n",
    "x.dot(y)                   # inner vector product / dot product\n",
    "x.t()                      # matrix transpose\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "In machine learning, **features** $x$ are typically described as a **column vector** of $n$ feature units:\n",
    "\n",
    "$$x = \\begin{pmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}$$\n",
    "\n",
    "Then a single neuron is the **inner product** or **dot product** `torch.dot()` (a.k.a. scalar product in a cartesian coordinate system) with a **column vector** of **weights** $w$\n",
    "\n",
    "$$w = \\begin{pmatrix}w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix}$$\n",
    "\n",
    "applied to an **activation function** $f$ which given a **scalar** $y$:\n",
    "\n",
    "$$y = f(w \\cdot x) = f(\\sum_{i=0}^n w_i x_i)$$\n",
    "\n",
    "In `PyTorch`, this is written as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n",
      "20\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.numpy().shape)\n",
    "\n",
    "w = torch.tensor([2, 3, 4])\n",
    "print(w.numpy().shape)\n",
    "\n",
    "y = torch.dot(w, x)               # alternatively w.dot(x)\n",
    "print(y.item())\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dot product** is **commutative**, hence we can also switch the order of the operands:\n",
    "\n",
    "$$y = f(x \\cdot w) = f(\\sum_{i=0}^n x_i w_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "y = torch.dot(x, w)               # alternatively x.dot(w)\n",
    "print(y.item())\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "If we describe features and weights with a $n \\times 1$ **column matrix**, we have \n",
    "\n",
    "$$x = \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\qquad \\text{and} \\qquad w = \\begin{bmatrix}w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix}$$\n",
    "\n",
    "and the dot product can be written as **matrix product** `torch.matmul()` using the **transpose** $w^\\top$ if the number of **columns** of the **left operand** is equal to the number of **rows** of the **right operand**:\n",
    "\n",
    "$$w^\\top \\cdot x = [w_1, \\ldots, w_n] \\cdot \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\sum_{i=0}^n w_i x_i$$\n",
    "\n",
    "If we use a **row matrix** $x = [x_1, \\ldots, x_n]$ for the features, we must switch the order of the operands to get the same scalar:\n",
    "\n",
    "$$x \\cdot w = [x_1, \\ldots, x_n] \\cdot \\begin{bmatrix}w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} = \\sum_{i=0}^n x_i w_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6614, 0.2669, 0.0617]])\n",
      "(1, 3)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "(3, 1)\n",
      "1.3802322149276733\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "w = torch.randn(3, 1)\n",
    "print(w.t())\n",
    "print(w.t().numpy().shape)\n",
    "\n",
    "x = torch.tensor([[1.], [2.], [3.]])\n",
    "print(x)\n",
    "print(x.numpy().shape)\n",
    "\n",
    "y = torch.matmul(w.t(), x)              # alternatively `w.t().mm(x)` or `w.t() @ x`\n",
    "print(y.item())\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As **matrix multiplication** is **non-communtative** $AB \\neq BA$, we must take care of the order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Product (German: Dyadisches Produkt)\n",
    "\n",
    "An outer product is a product of two vectors, or two one-column matrices, which results in a **matrix**:\n",
    "\n",
    "$$x \\otimes y = x \\cdot y^\\top = {\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_m \\end{bmatrix}} \\cdot [ y_1, \\cdots, y_n ] = \\begin{bmatrix} x_1 y_1 & \\cdots & x_1 y_n \\\\ \\vdots & & \\vdots \\\\ x_m y_1 & \\cdots & x_m y_n \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6614],\n",
      "        [0.2669],\n",
      "        [0.0617]])\n",
      "(3, 1)\n",
      "tensor([[1., 2., 3.]])\n",
      "(1, 3)\n",
      "tensor([[0.6614, 1.3227, 1.9841],\n",
      "        [0.2669, 0.5338, 0.8008],\n",
      "        [0.0617, 0.1234, 0.1850]])\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.numpy().shape)\n",
    "\n",
    "print(x.t())\n",
    "print(x.t().numpy().shape)\n",
    "\n",
    "y = torch.matmul(w, x.t())\n",
    "print(y)\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Networks\n",
    "\n",
    "When we have a multi layer neural network, the weights are typically a **matrix** $W$ with $n$ **rows** for $n$ **features** and $m$ **columns** for $m$ **hidden nodes**:\n",
    "\n",
    "$$W = \\begin{bmatrix}w_{11} & \\cdots & w_{1m} \\\\ \\vdots & & \\vdots \\\\ w_{n1} & \\cdots & w_{nm} \\end{bmatrix}$$\n",
    "\n",
    "Then the items of the hidden layer are a **column matrix**\n",
    "\n",
    "$$h = f(W^\\top x) = \\begin{bmatrix}h_1 \\\\ \\vdots \\\\ h_m \\end{bmatrix}$$\n",
    "\n",
    "with\n",
    "\n",
    "$$W^\\top x = \\begin{bmatrix}w_{11} & \\cdots & w_{n1} \\\\ \\vdots & & \\vdots \\\\ w_{1m} & \\cdots & w_{nm} \\end{bmatrix} \\cdot \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.0617, -0.4519],\n",
      "        [ 0.2669,  0.6213, -0.1661]])\n",
      "(2, 3)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "(3, 1)\n",
      "tensor([[-0.5710],\n",
      "        [ 1.0112]])\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "W = torch.randn(3, 2)\n",
    "print(W.t())\n",
    "print(W.t().numpy().shape)\n",
    "\n",
    "x = torch.tensor([[1.], [2.], [3.]])\n",
    "print(x)\n",
    "print(x.numpy().shape)\n",
    "\n",
    "y = torch.matmul(W.t(), x)\n",
    "print(y)\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of **columns** of $W$ is **not equal** the number of **rows** of $x$, we can't calculate $x \\cdot W^\\top$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3 x 1], m2: [2 x 3] at ../aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ac248ce1291b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [3 x 1], m2: [2 x 3] at ../aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "y = torch.matmul(x, W.t())\n",
    "print(y)\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as $(A \\cdot B)^T = B^T \\cdot A^T$ and $(A^\\top)^\\top = A$, we can caluclate\n",
    "\n",
    "$$x^\\top \\cdot W = ((x^\\top \\cdot W)^\\top)^\\top = (W^\\top \\cdot (x^\\top)^\\top)^\\top = (W^\\top \\cdot x)^\\top = h^\\top$$\n",
    "\n",
    "to get the **transpose** of the **hidden column matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5710,  1.0112]])\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "y = torch.matmul(x.t(), W)\n",
    "print(y)\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in PyTorch to use a **row matrix** for a feature vector, hence we calculate the **matrix product** `torch.matmul()` as $x \\cdot W$ to get a **hidden row matrix**:\n",
    "\n",
    "$$h = f(x \\cdot W) = [h_1, \\ldots, h_m]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Batches\n",
    "\n",
    "In PyTorch it is common to have a **batch** of $k$ **feature vectors**, which are described as a matrix having $k$ rows and $n$ columns. \n",
    "\n",
    "Each row represents one feature vector $x^{(i)} = (x_1^{(i)}, \\ldots, x_n^{(i)})$ (consider features as stacked within a matrix):\n",
    "\n",
    "$$X = \\begin{bmatrix}x^{(1)} \\\\ \\vdots \\\\ x^{(k)} \\end{bmatrix} = \\begin{bmatrix}x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ \\vdots & & \\vdots \\\\ x_1^{(k)} & \\cdots & x_n^{(k)} \\end{bmatrix} = \\begin{bmatrix}x_{11} & \\cdots & x_{1n} \\\\ \\vdots & & \\vdots \\\\ x_{k1} & \\cdots & x_{kn} \\end{bmatrix}$$\n",
    "\n",
    "With a **weights matrix** from a multi layer neural network\n",
    "\n",
    "$$W = \\begin{bmatrix}w_{11} & \\cdots & w_{1m} \\\\ \\vdots & & \\vdots \\\\ w_{n1} & \\cdots & w_{nm} \\end{bmatrix}$$\n",
    "\n",
    "we then calculate the items of the hidden layer as the $k \\times m$ **matrix product** of the $k \\times n$ **feature matrix** and the $n \\times m$ **weight matrix**:\n",
    "\n",
    "$$h = f(X \\cdot W) = \\begin{bmatrix}h_{11} & \\cdots & h_{1m} \\\\ \\vdots & & \\vdots \\\\ h_{k1} & \\cdots & h_{km} \\end{bmatrix} = \\begin{bmatrix}h^{(1)} \\\\ \\vdots \\\\ h^{(k)} \\end{bmatrix}$$\n",
    "\n",
    "with\n",
    "\n",
    "$$X \\cdot W = \\begin{bmatrix}x_{11} & \\cdots & x_{1n} \\\\ \\vdots & & \\vdots \\\\ x_{k1} & \\cdots & x_{kn} \\end{bmatrix} \\cdot \\begin{bmatrix}w_{11} & \\cdots & w_{1m} \\\\ \\vdots & & \\vdots \\\\ w_{n1} & \\cdots & w_{nm} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.]])\n",
      "(1, 3)\n",
      "tensor([[ 0.6614,  0.2669],\n",
      "        [ 0.0617,  0.6213],\n",
      "        [-0.4519, -0.1661]])\n",
      "(3, 2)\n",
      "tensor([[-0.5710,  1.0112]])\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "X = torch.tensor([[1., 2., 3.]])          # 1 batch, feature as a row-matrix\n",
    "print(X)\n",
    "print(X.numpy().shape)\n",
    "\n",
    "W = torch.randn(3, 2)                     # 3 input units, 2 hidden units\n",
    "print(W)\n",
    "print(W.numpy().shape)\n",
    "\n",
    "y = torch.matmul(X, W)\n",
    "print(y)\n",
    "print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Neural Network\n",
    "\n",
    "This is an example neural network with 3 **input units**, 2 **hidden units** and 1 **output unit** using a non-linearity as activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1766]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "X = torch.randn(1, 3)\n",
    "\n",
    "W_ih = torch.randn(3, 2)\n",
    "W_ho = torch.randn(2, 1)\n",
    "\n",
    "B_h = torch.randn(1, 2)\n",
    "B_o = torch.randn(1, 1)\n",
    "\n",
    "h = torch.sigmoid(torch.mm(X, W_ih) + B_h)\n",
    "y = torch.sigmoid(torch.mm(h, W_ho) + B_o)\n",
    "\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
