{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Softmax is an Activation Functions. Softmax is a generalization of Sigmoind for $n = 2$\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    TODO:\n",
    "    <br/>https://www.quora.com/Why-is-it-better-to-use-Softmax-function-than-sigmoid-function \n",
    "    <br/>https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "Sigmoid is an activation function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "The **derivative** of Sigmoid is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\sigma(x) &= \\frac{\\mathrm{d}}{\\mathrm{d}x} \\Big(\\frac{1}{1 + e^{-x}} \\Big)^{-1} \\\\[3pt]\n",
    "&= -(1 + e^{-x})^{-2} \\; (-e^{-x}) \\\\[3pt]\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^2} \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\frac{e^{-x}}{1+e^{-x}} \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\frac{1 + e^{-x} - 1}{1+e^{-x}} \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\bigg( \\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}} \\bigg) \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\bigg( 1 - \\frac{1}{1+e^{-x}} \\bigg) \\\\[3pt]\n",
    "&= \\sigma(x) \\cdot \\big( 1 - \\sigma(x) \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the **error**. \n",
    "\n",
    "Cross Entropy is used in ML as a **loss function**. Multiclass Cross Entropy is a generalization for Two Class Cross Entropy\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">TODO</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Squared Errors (SSE)\n",
    "\n",
    "A common metric is the sum of the squared errors. The **SSE** is a good choice for a few reasons, for example compared to just using $E=|y-\\hat{y}|$: the square ensures the **error is always positive** and **larger errors are penalized** more than smaller errors. Also, it makes the **math nice**.\n",
    "\n",
    "$$E = \\frac{1}{2} \\sum_\\mu \\sum_j \\big[y_j^{\\,\\mu} - \\hat{y}_j^{\\,\\mu}\\big]^2$$\n",
    "\n",
    "with the **prediction** \n",
    "\n",
    "$$\\hat{y}_j^{\\,\\mu} = f \\big(\\sum_i w_{ij} \\, x_i^{\\,\\mu} \\big) = f(h_{\\mu j})$$\n",
    "\n",
    "and the **real value** $y$, the number of **output units** $j$ and the number of **data records** $\\mu$. Also let be $h_{\\mu j}$ the **linear combination** of the weights and the inputs.\n",
    "\n",
    "Our goal is to find weights $w_{ij}$ that minimize the squared error $E$. To do this with a neural network we typically use **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent of SSE\n",
    "\n",
    "Weights are updated by **substracting** the **gradient** (or **weight step**) $\\Delta w_i$ multiplied by a **learning rate** $\\eta$:\n",
    "\n",
    "$$w_i  = w_i + \\Delta w_i = w_i - \\eta \\, \\frac{\\partial E}{\\partial w_i}$$\n",
    "\n",
    "### One Output Unit\n",
    "\n",
    "Supposed we have only **one data record** and **one output unit** then the SSE is\n",
    "\n",
    "$$E = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "with the **error** $(y - \\hat{y})$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\hat{y} &= f(h) = \\sigma(h) \\\\[3pt]\n",
    "        h &= \\sum_i w_i x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The gradient of $E$ w.r.t. $w_i$ is the negative of the **error** times the **derivative of the activation function** at $h$ times the **input value** $x_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial E}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\frac{1}{2} (y - \\hat{y})^2 \\\\[3pt]\n",
    "                                  &=(y - \\hat{y}) \\cdot \\frac{\\partial}{\\partial w_i} (y - \\hat{y}) \\\\[3pt]\n",
    "                                  &=-(y - \\hat{y}) \\cdot \\frac{\\partial}{\\partial w_i} \\hat{y} \\\\[3pt]\n",
    "                                  &=-(y - \\hat{y}) \\cdot f'(h) \\cdot \\frac{\\partial}{\\partial w_i}\\sum_i w_i x_i \\\\[3pt]\n",
    "                                  &=-(y - \\hat{y}) \\cdot f'(h) \\cdot x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then the **weight step** $\\Delta w_i$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Delta w_i &= - \\eta \\, \\frac{\\partial E}{\\partial w_i} \\\\[3pt]\n",
    "             &= \\eta \\cdot (y - \\hat{y}) \\cdot f'(h) \\cdot x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For convenience we define an **error term** $\\delta$ as\n",
    "\n",
    "$$\\delta = (y - \\hat{y}) \\cdot f'(h)$$\n",
    "\n",
    "so we can write the **weight update** as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   w_i &= w_i + \\Delta w_i \\\\[2pt]\n",
    "       &= w_i - \\eta \\, \\frac{\\partial E}{\\partial w_i} \\\\[2pt]\n",
    "       &= w_i + \\eta \\, \\delta \\, x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Multiple Output Units\n",
    "\n",
    "for multiple output units we have multiple error terms:\n",
    "\n",
    "$$\\delta_j = (y_j - \\hat{y}_j) \\cdot f'(h_j)$$\n",
    "\n",
    "$$w_{ij} = w_{ij} + \\Delta w_{ij} = w_{ij} + \\eta \\, \\delta_j \\, x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Example\n",
    "\n",
    "with one data record and four input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>-0.1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>-0.5406</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>-0.0475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>-0.5524</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.4734</td>\n",
       "      <td>-0.5533</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>-0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>-0.5533</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>-0.5533</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>-0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       w1      w2      w3      w4   error\n",
       "0  0.5000 -0.5000  0.3000  0.1000 -0.1900\n",
       "1  0.4797 -0.5406  0.2390  0.0187 -0.0475\n",
       "2  0.4738 -0.5524  0.2214 -0.0048 -0.0035\n",
       "3  0.4734 -0.5533  0.2201 -0.0065 -0.0002\n",
       "4  0.4733 -0.5533  0.2200 -0.0067 -0.0000\n",
       "5  0.4733 -0.5533  0.2200 -0.0067 -0.0000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])                # Initial weights\n",
    "\n",
    "data = []\n",
    "for i in range(6):\n",
    "    h = np.dot(x, w)\n",
    "    y_hat = sigmoid(h)\n",
    "    error = y - y_hat\n",
    "    data.append((w[0], w[1], w[2], w[3], error))\n",
    "\n",
    "    error_term = error * sigmoid_prime(h)          # more efficient would be: error_term = error * y_hat * (1 - y_hat)\n",
    "    delta_w = learnrate * error_term * x\n",
    "    w = w + delta_w\n",
    "\n",
    "pd.DataFrame(data, columns=['w1', 'w2', 'w3', 'w4', 'error']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Errors (MSE)\n",
    "\n",
    "When we're using a **lot of data**, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, we'd need to use a quite small learning rate. \n",
    "\n",
    "Instead, we can just **divide by the number of records** in our data, $m$ to take the average:\n",
    "\n",
    "$$E = \\frac{1}{2m} \\sum_\\mu^m \\big( y^{\\,\\mu} - \\hat{y}^{\\,\\mu}\\big)^2$$\n",
    "\n",
    "This way, no matter how much data we use, our **learning rates** will typically be in the range of **0.01** to **0.001**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Momentum\n",
    "\n",
    "[Momentum](https://distill.pub/2017/momentum/) is a method to **avoid local minimums** and miss the lowest possible minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "see also [this interactive description](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll) of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the Error Term with Scalars\n",
    "\n",
    "Supposed we have a neural network with a **single input layer** $x_1$, **two single hidden layers** $a_1$ and $a_2$ and a **single output layer** $\\hat y$ like this:\n",
    "\n",
    "![backpropagation](images/machine-learning-backpropagation.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden and output layer has a **sigmoid** function $f$ as an **activation function**.\n",
    "\n",
    "Let the **error** $E$ and the **loss function** $L$ be\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   E &= y - \\hat y \\\\[3pt]\n",
    "   L &= \\frac{1}{2} E^2 = \\frac{1}{2} (y - \\hat y)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and let $h_1 = w_1 x_1$, $a_1 = f(h_1)$, $h_2 = w_2 a_1$, $a_2 = f(h_2)$, $h_3 = w_3 a_2$ and $\\hat y = f(h_3)$.\n",
    "\n",
    "We then calculate the gradient of $E$ w.r.t. $w_i$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial L}{\\partial w_3} &= \\frac{\\partial L}{\\partial E} \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_3} \\frac{\\partial h_3}{\\partial w_3} \\\\[3pt]\n",
    "                                   &= (y - \\hat y)(-1)f'(h_3) \\, a_2 \\\\[3pt]\n",
    "                                   &= \\delta_3 a_2 \\\\[10pt]\n",
    "   \\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial E} \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_3} \\frac{\\partial h_3}{\\partial a_2} \\frac{\\partial a_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial w_2} \\\\[3pt]\n",
    "                                   &= \\delta_3 w_3 f'(h_2) \\, a_1 \\\\[3pt]\n",
    "                                   &= \\delta_2 a_1 \\\\[10pt]\n",
    "   \\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial E} \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_3} \\frac{\\partial h_3}{\\partial a_2} \\frac{\\partial a_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial a_1}  \\frac{\\partial a_1}{\\partial h_1}  \\frac{\\partial h_1}{\\partial w_1}\\\\[3pt]\n",
    "                                   &= \\delta_2 w_2 f'(h_1) \\, x_1 \\\\[3pt]\n",
    "                                   &= \\delta_1 x_1 \\\\[10pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\delta_3 &= -(y - \\hat y) \\, f'(h_3) = -(y - \\hat y) \\, \\hat y \\,(1 - \\hat y) \\\\[3pt]\n",
    "  {\\color{red}\\delta}_{\\color{red}2} &= {\\color{red}\\delta}_{\\color{red}3} \\, {\\color{red}w}_{\\color{red}3} \\, {\\color{red}f}'{\\color{red}(}{\\color{red}h}_{\\color{red}2}{\\color{red})} = \\delta_3 w_3 \\, a_2 \\, (1 - a_2) \\\\[3pt]\n",
    "  \\delta_1 &= \\delta_2 \\, w_2 \\, f'(h_1) = \\delta_2 \\, w_2 \\, a_1 \\, (1 - a_1) \\\\[3pt]\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify the calculations, we let **PyTorch autograd** calculate the gradients and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "x_1 = torch.tensor([0.5])                         # x_1 = 0.5\n",
    "y = torch.tensor([1.0])                           # y = 1\n",
    "\n",
    "w_1 = torch.tensor([1.2], requires_grad=True)     # w_1 = 1.2\n",
    "w_2 = torch.tensor([1.2], requires_grad=True)     # w_2 = 1.2\n",
    "w_3 = torch.tensor([1.2], requires_grad=True)     # w_3 = 1.2\n",
    "\n",
    "h_1 = w_1 * x_1                                   # h_1 = 0.6\n",
    "a_1 = sigmoid(h_1)                                # a_1 = 0.6457\n",
    "h_2 = w_2 * a_1                                   # h_2 = 0.7748\n",
    "a_2 = sigmoid(h_2)                                # a_2 = 0.6846\n",
    "h_3 = w_3 * a_2                                   # h_3 = 0.8215\n",
    "y_hat = sigmoid(h_3)                              # y_hat = 0.6945\n",
    "\n",
    "E = y - y_hat                                     # E = 0.3055\n",
    "L = 0.5 * E ** 2                                  # L = 0.0467\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0444])\n",
      "tensor([-0.0108])\n",
      "tensor([-0.0023])\n"
     ]
    }
   ],
   "source": [
    "d_3 = -1 * E * y_hat * (1 - y_hat)\n",
    "grad_w_3 = d_3 * a_2                              # grad_w_3 = -0.0444\n",
    "print(grad_w_3.data)\n",
    "\n",
    "d_2 = d_3 * w_3 * a_2 * (1 - a_2)\n",
    "grad_w_2 = d_2 * a_1                              # grad_w_2 = -0.0108\n",
    "print(grad_w_2.data)\n",
    "\n",
    "d_1 = d_2 * w_2 * a_1 * (1 - a_1)\n",
    "grad_w_1 = d_1 * x_1                              # grad_w_1 = -0.0023\n",
    "print(grad_w_1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0444])\n",
      "tensor([-0.0108])\n",
      "tensor([-0.0023])\n"
     ]
    }
   ],
   "source": [
    "print(w_3.grad)\n",
    "print(w_2.grad)\n",
    "print(w_1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the Error Term with Tensors\n",
    "\n",
    "![](images/machine-learning-backprop-tensor.svg)\n",
    "\n",
    "(the indexes describe <span style=\"background: #ffff0080;\">the layer - not the layer unit</span>)\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "The **error term** $\\delta_k$ for the output layer $k$ is\n",
    "\n",
    "$$\\delta_k = -(y - \\hat y\\,) \\, f'(h_k) \\qquad \\text{with} \\; h_k = w_{jk}^\\top h_j \\quad \\text{and} \\; f(h_k) = \\hat y$$\n",
    "\n",
    "hence, the **weight change** $\\Delta w_{jk}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Delta w_{jk} &= \\delta_k \\, f(h_j) \\\\[2pt]\n",
    "                &= (\\hat y - y) \\, f'(h_k) \\, f(h_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Hidden Layer\n",
    "\n",
    "The **error term** $\\delta_j$ for the hidden layer $j$ is\n",
    "\n",
    "$$\\delta_j = w_{jk}^\\top \\delta_k \\, f'(h_j) \\qquad \\text{with}  \\; h_j = w_{ij}^\\top h_i$$\n",
    "\n",
    "hence the **weight change** $\\Delta w_{ij}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Delta w_{ij} &= \\delta_j \\, f(h_i) \\\\[2pt]\n",
    "                &= w_{jk}^\\top \\delta_k \\, f'(h_j) \\, f(h_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify the calculations, we let **PyTorch autograd** calculate the gradients and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "x = torch.tensor([0.5, 0.8, 1.2])                      # shape: (3,)\n",
    "y = torch.tensor([1.0, 0.0])                           # shape: (2,)\n",
    "\n",
    "w_1 = torch.ones((3, 3), requires_grad=True)           # shape: (3,3)\n",
    "w_2 = torch.ones((3, 2), requires_grad=True)           # shape: (3,2)\n",
    "w_3 = torch.ones((2, 2), requires_grad=True)           # shape: (2,2)\n",
    "\n",
    "h_1 = w_1.T.mv(x)                                      # shape: (3,)                       h_1   = [2.5, 2.5, 2.5]\n",
    "a_1 = sigmoid(h_1)                                     # shape: (3,)                       a_1   = [0.9241, 0.9241, 0.9241]\n",
    "h_2 = w_2.T.mv(a_1)                                    # shape: (2,)                       h_2   = [2.7724, 2.7724]\n",
    "a_2 = sigmoid(h_2)                                     # shape: (2,)                       a_2   = [0.9412, 0.9412]\n",
    "h_3 = w_3.T.mv(a_2)                                    # shape: (2,)                       h_3   = [1.8823, 1.8823]\n",
    "y_hat = sigmoid(h_3)                                   # shape: (2,)                       y_hat = [0.8679, 0.8679]\n",
    "\n",
    "E = y - y_hat                                          # shape: (2,)                       E     = [0.1321, -0.8679]\n",
    "L = 0.5 * E.dot(E)                                     # shape: ()                         L     = [0.3853]\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0143,  0.0937],\n",
      "        [-0.0143,  0.0937]])\n",
      "tensor([[0.0043, 0.0043],\n",
      "        [0.0043, 0.0043],\n",
      "        [0.0043, 0.0043]])\n",
      "tensor([[0.0003, 0.0003, 0.0003],\n",
      "        [0.0005, 0.0005, 0.0005],\n",
      "        [0.0008, 0.0008, 0.0008]])\n"
     ]
    }
   ],
   "source": [
    "# delta_3\n",
    "d_3 = -1 * E * y_hat * (1 - y_hat)                     # shape: (2,)                                 d_3 = [-0.0151, 0.0995]\n",
    "grad_w_3 = (d_3.view(-1, 1) @ a_2.view(1, -1)).T       # shape: (2,1) @ (1,2) -> (2,2) -> (2,2)\n",
    "print(grad_w_3.data)\n",
    "\n",
    "d_2 = d_3 @ w_3.T * a_2 * (1 - a_2)                    # shape: (2,)                                 d_2 = [0.0047, 0.0047]\n",
    "grad_w_2 = (d_2.view(-1, 1) @ a_1.view(1, -1)).T       # shape: (2,1) @ (1,3) -> (2,3) -> (3,2)\n",
    "print(grad_w_2.data)\n",
    "\n",
    "d_1 = d_2 @ w_2.T * a_1 * (1 - a_1)                    # shape: (3,)                                 d_1 = [0.0007, 0.0007, 0.0007]\n",
    "grad_w_1 = (d_1.view(-1, 1) * x.view(1, -1)).T         # shape: (3,1) @ (1,3) -> (3,3) -> (3,3)\n",
    "print(grad_w_1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0143,  0.0937],\n",
      "        [-0.0143,  0.0937]])\n",
      "tensor([[0.0043, 0.0043],\n",
      "        [0.0043, 0.0043],\n",
      "        [0.0043, 0.0043]])\n",
      "tensor([[0.0003, 0.0003, 0.0003],\n",
      "        [0.0005, 0.0005, 0.0005],\n",
      "        [0.0008, 0.0008, 0.0008]])\n"
     ]
    }
   ],
   "source": [
    "print(w_3.grad)\n",
    "print(w_2.grad)\n",
    "print(w_1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "In machine learning **convolution** is a mathematical operation on two discrete functions (an input $\\text{I}$ and a kernel $\\text{K}$) that produces a third discrete function (an output $\\text{O}$) expressing how the shape of $\\text{I}$ is modified by $\\text{K}$:\n",
    "\n",
    "$$\\text{I} \\ast \\text{K} =  \\text{O}$$\n",
    "\n",
    "Computing the inverse of the convolution operation is known as **deconvolution**: finding an input $\\text{I}$ that satisfies the equation $\\text{I} \\ast \\text{K} =  \\text{O}$.\n",
    "\n",
    "The term **transposed convolution** describes exactly what a standard convolutional layer does, but on a **modified input** feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "A **standard convolution** is typically used for **downsampling** a given input. \n",
    "\n",
    "The convolutional layer of an **input** of size $i \\times i$ is defined by the parameters:\n",
    "\n",
    "1. **Padding** $p$: The amount of zeros padded around the original input increasing the size to $(i+2p) \\times (i+2p)$\n",
    "2. **Stride** $s$: The amount by which the kernel is shifted when sliding across the input image.\n",
    "3. **Insertions** $z$: filling the layer with zeros. For convolutions there is no filling: $z=0$.\n",
    "\n",
    "For a given size of input $i$, kernel $k$, padding $p$ and stride $s$, the size of the output feature map $o \\times o$ is given by\n",
    "\n",
    "$$o = \\frac{i + 2p - k}{s} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(1, 1, 3, stride=3, padding=1)      # k=3, s=3, p=1\n",
    "x = torch.ones(1, 1, 10, 10)                        # i=10  \n",
    "\n",
    "conv(x).squeeze(0).squeeze(0).shape                 # squeeze away batch- and color-depth dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolutional Layer\n",
    "\n",
    "**Transposed convolutions** are used for **upsampling** a given input (which is typically the output of a standard convolution).\n",
    "\n",
    "A transposed convolutional layer is also defined by the padding and stride. These values of padding and stride are the one that hypothetically were carried out on the output to generate the input: if you take the output, and carry out a standard convolution with stride and padding defined, it will generate the spatial dimension same as that of the input.\n",
    "\n",
    "For a transposed convolutional layer of an **input** size $i \\times i$, we calculate:\n",
    "\n",
    "1. **Padding** $p\\,' = k - p - 1$\n",
    "2. **Stride** $s\\,' = 1$\n",
    "3. **Insertions** $z = s -1 $\n",
    "\n",
    "For a given size of input $i$, kernel $k$, padding $p$ and stride $s$, the size of the output feature map $o \\times o$ generated is given by\n",
    "\n",
    "$$o = (i-1) \\cdot s + k - 2p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Image Source: [Animation for Convolutional layers](https://github.com/thomd/conv_layers_animation)\n",
    "\n",
    "<img src=\"images/t_conv_K3S2P1.gif\" width=\"500\" style=\"float:left\"/>\n",
    "<img src=\"images/t_conv_K3S3P1.gif\" width=\"500\" style=\"float:left\"/>\n",
    "<img src=\"images/t_conv_K3S4P1.gif\" width=\"500\" style=\"float:left\"/>\n",
    "<img src=\"images/t_conv_K3S2P0.gif\" width=\"500\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "t_conv = nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1)      # k=3, s=2, p=1\n",
    "x = torch.ones(1, 1, 3, 3)                                     # i=3\n",
    "\n",
    "t_conv(x).squeeze(0).squeeze(0).shape                          # squeeze away batch- and color-depth dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Gram matrix** $G$ of a set of vectors $a_1, \\dots ,a_n$ is the Hermitian matrix of **inner products** (or **dot products**), whose entries $g_{ij}$ are given by\n",
    "\n",
    "$$g_{ij} = a_i \\cdot a_j$$\n",
    "\n",
    "if vectors $a_1, \\dots ,a_n$ are **columns** of a **matrix** $A$, then\n",
    "\n",
    "$$G = A^\\top \\cdot A$$\n",
    "\n",
    "A Gram matrix is **Positive Definite** and **Symmetric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "The dot product $\\vec{a} \\cdot \\vec{b} = \\vert a \\vert \\vert b \\vert \\cos(\\theta)$ can be seen as **how similar two vectors** actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let \n",
    "\n",
    "$$\\vec{a}_1 = \\begin{pmatrix}a_{11}\\\\a_{21}\\\\a_{31}\\\\a_{41}\\\\\\end{pmatrix} = \\begin{pmatrix}1\\\\2\\\\3\\\\4\\\\\\end{pmatrix} \\; \\text{and} \\;\\; \\vec{a}_2 = \\begin{pmatrix}a_{21}\\\\a_{22}\\\\a_{32}\\\\a_{42}\\\\\\end{pmatrix} = \\begin{pmatrix}2\\\\3\\\\4\\\\5\\\\\\end{pmatrix}$$\n",
    "\n",
    "then the Gram Matrix calculates as\n",
    "\n",
    "$$G(\\vec{a}_1, \\vec{a}_2) = (\\vec{a}_1, \\vec{a}_2)^\\top \\cdot (\\vec{a}_1, \\vec{a}_2) = \\begin{pmatrix} \\vec{a}_1 \\\\ \\vec{a}_2 \\end{pmatrix} \\cdot (\\vec{a}_1, \\vec{a}_2) = \\begin{pmatrix}\\vec{a}_1 \\cdot \\vec{a}_1 & \\vec{a}_1 \\cdot \\vec{a}_2\\\\\\vec{a}_2 \\cdot \\vec{a}_1 & \\vec{a}_2 \\cdot \\vec{a}_2\\end{pmatrix} = \\begin{pmatrix}30 & 40\\\\40 & 54\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 3],\n",
       "       [3, 4],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a1 = np.array([1, 2, 3, 4])\n",
    "a2 = np.array([2, 3, 4, 5])\n",
    "\n",
    "A = np.column_stack((a1, a2))    # column_stack(): vectors are columns\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30, 40],\n",
       "       [40, 54]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.matmul(A.T, A)\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram Matrix in Convolutional Layers\n",
    "\n",
    "In machine learning, a specific **convolutions layer** $l$ with $N_l$ distinct **filters** has $N_l$ **vectorized feature maps** each of size $M_l = w \\cdot h$ with $w$ and $h$ the width and height of the input image (hence number of pixels). A convolutional layer can hence be described as a matrix $F^l \\in \\textbf{R}^{N_l \\times M_l}$.\n",
    "\n",
    "**Feature correlations** are given by the **Gram matric** $G^l \\in \\textbf{R}^{N_l \\times N_l}$, where $G_{ij}^l$ is the inner product between the vectorized feature maps $i$ and $j$ in layer $l$:\n",
    "\n",
    "$$G_{ij}^l = \\sum_{k=1}^{M_l} F_{ik}^l F_{jk}^l \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "The gram matrix describe the **correlation** or **similarity** between $N_l$ **filters** of an convolutional layer $l$ if we consider the feature map as $N_l$ filters $K$ applied to a vectorized input image of size $M_l$.\n",
    "\n",
    "The formula for gram matrix (1) reminds to the calculation of a **correlation** $r$ except that we dont substract the mean and don't divide by the vector lengths:\n",
    "\n",
    "$$r = \\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_i (x_i - \\bar{x})^2} \\sqrt{\\sum_i (y_i - \\bar{y})^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with PyTorch\n",
    "\n",
    "Using the example above, we have two vectorized filter maps $\\vec{a}_i$ of a $2 \\times 2$ image, hence a convolutional layer \n",
    "\n",
    "$$F = (\\vec{a}_1, \\vec{a}_2) = \\begin{pmatrix}1 & 2\\\\2 & 3\\\\3 & 4\\\\4 & 5\\\\\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
    "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the batch_size, depth, height, and width of the Tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    # vectorize the feature map\n",
    "    tensor = tensor.view(d, h * w)\n",
    "\n",
    "    # calculate the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [4, 5]]])\n",
      "torch.Size([1, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[30, 40],\n",
       "        [40, 54]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = torch.tensor([1, 2, 3, 4]).view(2,2)\n",
    "a2 = torch.tensor([2, 3, 4, 5]).view(2,2)\n",
    "\n",
    "A = torch.stack((a1, a2), dim=0)                 # dim=1: row stack\n",
    "print(A)\n",
    "\n",
    "A = A[None,:]                                    # add batch dimension\n",
    "print(A.shape)\n",
    "\n",
    "gram_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
