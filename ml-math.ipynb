{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learnign Math\n",
    "\n",
    "## Softmax\n",
    "\n",
    "Softmax is an Activation Functions. Softmax is a generalization of Sigmoind for $n = 2$\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">TODO: https://www.quora.com/Why-is-it-better-to-use-Softmax-function-than-sigmoid-function</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "Sigmoid is an activation function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "The **derivative** of Sigmoid is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\sigma(x) &= \\frac{\\mathrm{d}}{\\mathrm{d}x} \\Big(\\frac{1}{1 + e^{-x}} \\Big)^{-1} \\\\[3pt]\n",
    "&= -(1 + e^{-x})^{-2} \\; (-e^{-x}) \\\\[3pt]\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^2} \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\frac{e^{-x}}{1+e^{-x}} \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\frac{1 + e^{-x} - 1}{1+e^{-x}} \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\bigg( \\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}} \\bigg) \\\\[3pt]\n",
    "&= \\frac{1}{1+e^{-x}} \\cdot \\bigg( 1 - \\frac{1}{1+e^{-x}} \\bigg) \\\\[3pt]\n",
    "&= \\sigma(x) \\cdot \\big( 1 - \\sigma(x) \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the **error**. \n",
    "\n",
    "Cross Entropy is used in ML as a **loss function**. Multiclass Cross Entropy is a generalization for Two Class Cross Entropy\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">TODO</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Squared Errors (SSE)\n",
    "\n",
    "A common metric is the sum of the squared errors. The **SSE** is a good choice for a few reasons, for example compared to just using $E=|y-\\hat{y}|$: the square ensures the **error is always positive** and **larger errors are penalized** more than smaller errors. Also, it makes the **math nice**.\n",
    "\n",
    "$$E = \\frac{1}{2} \\sum_\\mu \\sum_j \\big[y_j^{\\,\\mu} - \\hat{y}_j^{\\,\\mu}\\big]^2$$\n",
    "\n",
    "with the **prediction** \n",
    "\n",
    "$$\\hat{y}_j^{\\,\\mu} = f \\big(\\sum_i w_{ij} \\, x_i^{\\,\\mu} \\big) = f(h_{\\mu j})$$\n",
    "\n",
    "and the **real value** $y$, the number of **output units** $j$ and the number of **data records** $\\mu$. Also let be $h_{\\mu j}$ the **linear combination** of the weights and the inputs.\n",
    "\n",
    "Our goal is to find weights $w_{ij}$ that minimize the squared error $E$. To do this with a neural network we typically use **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent of SSE\n",
    "\n",
    "Weights are updated by **substracting** the **gradient** (or **weight step**) $\\Delta w_i$ multiplied by a **learning rate** $\\eta$:\n",
    "\n",
    "$$w_i  = w_i + \\Delta w_i = w_i - \\eta \\, \\frac{\\partial E}{\\partial w_i}$$\n",
    "\n",
    "### One Output Unit\n",
    "\n",
    "Supposed we have only **one data record** and **one output unit** then the SSE is\n",
    "\n",
    "$$E = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "with the **error** $(y - \\hat{y})$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\hat{y} &= f(h) = \\sigma(h) \\\\[3pt]\n",
    "        h &= \\sum_i w_i x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The gradient of $E$ w.r.t. $w_i$ is the negative of the **error** times the **derivative of the activation function** at $h$ times the **input value** $x_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial E}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\frac{1}{2} (y - \\hat{y})^2 \\\\[3pt]\n",
    "                                  &=(y - \\hat{y}) \\cdot \\frac{\\partial}{\\partial w_i} (y - \\hat{y}) \\\\[3pt]\n",
    "                                  &=-(y - \\hat{y}) \\cdot \\frac{\\partial}{\\partial w_i} \\hat{y} \\\\[3pt]\n",
    "                                  &=-(y - \\hat{y}) \\cdot f'(h) \\cdot \\frac{\\partial}{\\partial w_i}\\sum_i w_i x_i \\\\[3pt]\n",
    "                                  &=-(y - \\hat{y}) \\cdot f'(h) \\cdot x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then the **weight step** $\\Delta w_i$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Delta w_i &= - \\eta \\, \\frac{\\partial E}{\\partial w_i} \\\\[3pt]\n",
    "             &= \\eta \\cdot (y - \\hat{y}) \\cdot f'(h) \\cdot x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For convenience we define an **error term** $\\delta$ as\n",
    "\n",
    "$$\\delta = (y - \\hat{y}) \\cdot f'(h)$$\n",
    "\n",
    "so we can write the **weight update** as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   w_i &= w_i + \\Delta w_i \\\\[2pt]\n",
    "       &= w_i - \\eta \\, \\frac{\\partial E}{\\partial w_i} \\\\[2pt]\n",
    "       &= w_i + \\eta \\, \\delta \\, x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Multiple Output Units\n",
    "\n",
    "for multiple output units we have multiple error terms:\n",
    "\n",
    "$$\\delta_j = (y_j - \\hat{y}_j) \\cdot f'(h_j)$$\n",
    "\n",
    "$$w_{ij} = w_{ij} + \\Delta w_{ij} = w_{ij} + \\eta \\, \\delta_j \\, x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Example\n",
    "\n",
    "with one data record and four input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>-0.1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>-0.5406</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>-0.0475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>-0.5524</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.4734</td>\n",
       "      <td>-0.5533</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>-0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>-0.5533</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>-0.5533</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>-0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       w1      w2      w3      w4   error\n",
       "0  0.5000 -0.5000  0.3000  0.1000 -0.1900\n",
       "1  0.4797 -0.5406  0.2390  0.0187 -0.0475\n",
       "2  0.4738 -0.5524  0.2214 -0.0048 -0.0035\n",
       "3  0.4734 -0.5533  0.2201 -0.0065 -0.0002\n",
       "4  0.4733 -0.5533  0.2200 -0.0067 -0.0000\n",
       "5  0.4733 -0.5533  0.2200 -0.0067 -0.0000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])                # Initial weights\n",
    "\n",
    "data = []\n",
    "for i in range(6):\n",
    "    h = np.dot(x, w)\n",
    "    y_hat = sigmoid(h)\n",
    "    error = y - y_hat\n",
    "    data.append((w[0], w[1], w[2], w[3], error))\n",
    "\n",
    "    error_term = error * sigmoid_prime(h)          # more efficient would be: error_term = error * y_hat * (1 - y_hat)\n",
    "    delta_w = learnrate * error_term * x\n",
    "    w = w + delta_w\n",
    "\n",
    "pd.DataFrame(data, columns=['w1', 'w2', 'w3', 'w4', 'error']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Errors (MSE)\n",
    "\n",
    "When we're using a **lot of data**, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, we'd need to use a quite small learning rate. \n",
    "\n",
    "Instead, we can just **divide by the number of records** in our data, $m$ to take the average:\n",
    "\n",
    "$$E = \\frac{1}{2m} \\sum_\\mu^m \\big( y^{\\,\\mu} - \\hat{y}^{\\,\\mu}\\big)^2$$\n",
    "\n",
    "This way, no matter how much data we use, our **learning rates** will typically be in the range of **0.01** to **0.001**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Momentum\n",
    "\n",
    "[Momentum](https://distill.pub/2017/momentum/) is a method to **avoid local minimums** and miss the lowest possible minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "see also [this interactive description](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll) of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the Error Term with Scalars\n",
    "\n",
    "Supposed we have a neural network with a **single input layer** $x_1$, **two single hidden layers** $a_1$ and $a_2$ and a **single output layer** $\\hat y$ like this:\n",
    "\n",
    "![backpropagation](images/machine-learning-backpropagation.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden and output layer has a **sigmoid** function $f$ as an **activation function**.\n",
    "\n",
    "Let the **error** $E$ and the **loss function** $L$ be\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   E &= y - \\hat y \\\\[3pt]\n",
    "   L &= \\frac{1}{2} E^2 = \\frac{1}{2} (y - \\hat y)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and let $h_1 = w_1 x_1$, $a_1 = f(h_1)$, $h_2 = w_2 a_1$, $a_2 = f(h_2)$, $h_3 = w_3 a_2$ and $\\hat y = f(h_3)$.\n",
    "\n",
    "We then calculate the gradient of $E$ w.r.t. $w_i$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial L}{\\partial w_3} &= \\frac{\\partial L}{\\partial E} \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_3} \\frac{\\partial h_3}{\\partial w_3} \\\\[3pt]\n",
    "                                   &= (y - \\hat y)(-1)f'(h_3) \\, a_2 \\\\[3pt]\n",
    "                                   &= \\delta_3 a_2 \\\\[10pt]\n",
    "   \\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial E} \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_3} \\frac{\\partial h_3}{\\partial a_2} \\frac{\\partial a_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial w_2} \\\\[3pt]\n",
    "                                   &= \\delta_3 w_3 f'(h_2) \\, a_1 \\\\[3pt]\n",
    "                                   &= \\delta_2 a_1 \\\\[10pt]\n",
    "   \\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial E} \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_3} \\frac{\\partial h_3}{\\partial a_2} \\frac{\\partial a_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial a_1}  \\frac{\\partial a_1}{\\partial h_1}  \\frac{\\partial h_1}{\\partial w_1}\\\\[3pt]\n",
    "                                   &= \\delta_2 w_2 f'(h_1) \\, x_1 \\\\[3pt]\n",
    "                                   &= \\delta_1 x_1 \\\\[10pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\delta_3 &= -(y - \\hat y) \\, f'(h_3) = -(y - \\hat y) \\, \\hat y \\,(1 - \\hat y) \\\\[3pt]\n",
    "  {\\color{red}\\delta}_{\\color{red}2} &= {\\color{red}\\delta}_{\\color{red}3} \\, {\\color{red}w}_{\\color{red}3} \\, {\\color{red}f}'{\\color{red}(}{\\color{red}h}_{\\color{red}2}{\\color{red})} = \\delta_3 w_3 \\, a_2 \\, (1 - a_2) \\\\[3pt]\n",
    "  \\delta_1 &= \\delta_2 \\, w_2 \\, f'(h_1) = \\delta_2 \\, w_2 \\, a_1 \\, (1 - a_1) \\\\[3pt]\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify the calculations, we let **PyTorch autograd** calculate the gradients and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "x_1 = torch.tensor([0.5])                         # x_1 = 0.5\n",
    "y = torch.tensor([1.0])                           # y = 1\n",
    "\n",
    "w_1 = torch.tensor([1.2], requires_grad=True)     # w_1 = 1.2\n",
    "w_2 = torch.tensor([1.2], requires_grad=True)     # w_2 = 1.2\n",
    "w_3 = torch.tensor([1.2], requires_grad=True)     # w_3 = 1.2\n",
    "\n",
    "h_1 = w_1 * x_1                                   # h_1 = 0.6\n",
    "a_1 = sigmoid(h_1)                                # a_1 = 0.6457\n",
    "h_2 = w_2 * a_1                                   # h_2 = 0.7748\n",
    "a_2 = sigmoid(h_2)                                # a_2 = 0.6846\n",
    "h_3 = w_3 * a_2                                   # h_3 = 0.8215\n",
    "y_hat = sigmoid(h_3)                              # y_hat = 0.6945\n",
    "\n",
    "E = y - y_hat                                     # E = 0.3055\n",
    "L = 0.5 * E ** 2                                  # L = 0.0467\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0444])\n",
      "tensor([-0.0108])\n",
      "tensor([-0.0023])\n"
     ]
    }
   ],
   "source": [
    "d_3 = -1 * E * y_hat * (1 - y_hat)\n",
    "grad_w_3 = d_3 * a_2                              # grad_w_3 = -0.0444\n",
    "print(grad_w_3.data)\n",
    "\n",
    "d_2 = d_3 * w_3 * a_2 * (1 - a_2)\n",
    "grad_w_2 = d_2 * a_1                              # grad_w_2 = -0.0108\n",
    "print(grad_w_2.data)\n",
    "\n",
    "d_1 = d_2 * w_2 * a_1 * (1 - a_1)\n",
    "grad_w_1 = d_1 * x_1                              # grad_w_1 = -0.0023\n",
    "print(grad_w_1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0444])\n",
      "tensor([-0.0108])\n",
      "tensor([-0.0023])\n"
     ]
    }
   ],
   "source": [
    "print(w_3.grad)\n",
    "print(w_2.grad)\n",
    "print(w_1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the Error Term with Tensors\n",
    "\n",
    "![](images/machine-learning-backprop-tensor.svg)\n",
    "\n",
    "(the indexes describe <span style=\"background: #ffff0080;\">the layer - not the layer unit</span>)\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "The **error term** $\\delta_k$ for the output layer $k$ is\n",
    "\n",
    "$$\\delta_k = -(y - \\hat y\\,) \\, f'(h_k) \\qquad \\text{with} \\; h_k = w_{jk}^\\top h_j \\quad \\text{and} \\; f(h_k) = \\hat y$$\n",
    "\n",
    "hence, the **weight change** $\\Delta w_{jk}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Delta w_{jk} &= \\delta_k \\, f(h_j) \\\\[2pt]\n",
    "                &= (\\hat y - y) \\, f'(h_k) \\, f(h_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Hidden Layer\n",
    "\n",
    "The **error term** $\\delta_j$ for the hidden layer $j$ is\n",
    "\n",
    "$$\\delta_j = w_{jk}^\\top \\delta_k \\, f'(h_j) \\qquad \\text{with}  \\; h_j = w_{ij}^\\top h_i$$\n",
    "\n",
    "hence the **weight change** $\\Delta w_{ij}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Delta w_{ij} &= \\delta_j \\, f(h_i) \\\\[2pt]\n",
    "                &= w_{jk}^\\top \\delta_k \\, f'(h_j) \\, f(h_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify the calculations, we let **PyTorch autograd** calculate the gradients and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "x = torch.tensor([0.5, 0.8, 1.2])                      # shape: (3,)\n",
    "y = torch.tensor([1.0, 0.0])                           # shape: (2,)\n",
    "\n",
    "w_1 = torch.ones((3, 3), requires_grad=True)           # shape: (3,3)\n",
    "w_2 = torch.ones((3, 2), requires_grad=True)           # shape: (3,2)\n",
    "w_3 = torch.ones((2, 2), requires_grad=True)           # shape: (2,2)\n",
    "\n",
    "h_1 = w_1.T.mv(x)                                      # shape: (3,)                       h_1   = [2.5, 2.5, 2.5]\n",
    "a_1 = sigmoid(h_1)                                     # shape: (3,)                       a_1   = [0.9241, 0.9241, 0.9241]\n",
    "h_2 = w_2.T.mv(a_1)                                    # shape: (2,)                       h_2   = [2.7724, 2.7724]\n",
    "a_2 = sigmoid(h_2)                                     # shape: (2,)                       a_2   = [0.9412, 0.9412]\n",
    "h_3 = w_3.T.mv(a_2)                                    # shape: (2,)                       h_3   = [1.8823, 1.8823]\n",
    "y_hat = sigmoid(h_3)                                   # shape: (2,)                       y_hat = [0.8679, 0.8679]\n",
    "\n",
    "E = y - y_hat                                          # shape: (2,)                       E     = [0.1321, -0.8679]\n",
    "L = 0.5 * E.dot(E)                                     # shape: ()                         L     = [0.3853]\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0143,  0.0937],\n",
      "        [-0.0143,  0.0937]])\n",
      "tensor([[0.0043, 0.0043],\n",
      "        [0.0043, 0.0043],\n",
      "        [0.0043, 0.0043]])\n",
      "tensor([[0.0003, 0.0003, 0.0003],\n",
      "        [0.0005, 0.0005, 0.0005],\n",
      "        [0.0008, 0.0008, 0.0008]])\n"
     ]
    }
   ],
   "source": [
    "# delta_3\n",
    "d_3 = -1 * E * y_hat * (1 - y_hat)                     # shape: (2,)                                 d_3 = [-0.0151, 0.0995]\n",
    "grad_w_3 = (d_3.view(-1, 1) @ a_2.view(1, -1)).T       # shape: (2,1) @ (1,2) -> (2,2) -> (2,2)\n",
    "print(grad_w_3.data)\n",
    "\n",
    "d_2 = d_3 @ w_3.T * a_2 * (1 - a_2)                    # shape: (2,)                                 d_2 = [0.0047, 0.0047]\n",
    "grad_w_2 = (d_2.view(-1, 1) @ a_1.view(1, -1)).T       # shape: (2,1) @ (1,3) -> (2,3) -> (3,2)\n",
    "print(grad_w_2.data)\n",
    "\n",
    "d_1 = d_2 @ w_2.T * a_1 * (1 - a_1)                    # shape: (3,)                                 d_1 = [0.0007, 0.0007, 0.0007]\n",
    "grad_w_1 = (d_1.view(-1, 1) * x.view(1, -1)).T         # shape: (3,1) @ (1,3) -> (3,3) -> (3,3)\n",
    "print(grad_w_1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0143,  0.0937],\n",
      "        [-0.0143,  0.0937]])\n",
      "tensor([[0.0043, 0.0043],\n",
      "        [0.0043, 0.0043],\n",
      "        [0.0043, 0.0043]])\n",
      "tensor([[0.0003, 0.0003, 0.0003],\n",
      "        [0.0005, 0.0005, 0.0005],\n",
      "        [0.0008, 0.0008, 0.0008]])\n"
     ]
    }
   ],
   "source": [
    "print(w_3.grad)\n",
    "print(w_2.grad)\n",
    "print(w_1.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
